{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7584f15-80ca-4b56-8cb2-2bc459f50120",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PyTorch Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118e764-4fec-4f0d-ada6-f54cb32a14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import decode_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "img = decode_image(\"pexels-jeffrey-czum-2346165.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\",\n",
    "                          width=4, font=\"Helvetica.ttf\", font_size=30)\n",
    "im = to_pil_image(box.detach())\n",
    "im.resize([i//3 for i in im.size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf79f2-767c-4161-a87f-2fe073f1222f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Native Python PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44635f8e-c1b6-44bb-86df-67006ec8c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from torchvision.io.image import decode_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection import fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "def batched(array, size):\n",
    "    array_iter = iter(array) \n",
    "    while True: \n",
    "        b = []\n",
    "        try: [b.append(next(array_iter)) for _ in range(size)]\n",
    "        except: StopIteration\n",
    "        if b: yield b \n",
    "        else: break\n",
    "\n",
    "def annotate(img, prediction, categories):\n",
    "    labels = [categories[i] for i in prediction[\"labels\"]]\n",
    "    box = draw_bounding_boxes(\n",
    "        image=img, \n",
    "        boxes=prediction['boxes'], \n",
    "        labels=labels, \n",
    "        colors=\"red\", \n",
    "        width=4, \n",
    "        font=\"Helvetica.ttf\" if os.path.exists(\"Helvetica.ttf\") else None, \n",
    "        font_size=30\n",
    "    )\n",
    "    im = to_pil_image(box.detach())\n",
    "    return im\n",
    "\n",
    "def detect_annotate_save(images, save_files, preprocess, model, categories, thresh=0.5): \n",
    "    processed_images = [preprocess(image) for image in images]\n",
    "    predictions = model(processed_images) \n",
    "    \n",
    "    for image, prediction, fname in zip(images, predictions, save_files): \n",
    "        prediction = {\n",
    "            k: prediction[k][prediction['scores']>thresh] for k in prediction\n",
    "        }\n",
    "        im = annotate(image, prediction, categories)\n",
    "        im.save(fname)\n",
    "\n",
    "def annotate_batch(images, predictions, categories, thresh=0.5): \n",
    "    annotated_images = [] \n",
    "    for image, prediction in zip(images, predictions): \n",
    "        prediction = {\n",
    "            k: prediction[k][prediction['scores']>thresh] for k in prediction\n",
    "        }\n",
    "        annotated_images.append(annotate(image, prediction, categories)) \n",
    "    return annotated_images\n",
    "\n",
    "def get_model_and_weights(builder_func=fcos_resnet50_fpn, weight_class=FCOS_ResNet50_FPN_Weights): \n",
    "    weights = weight_class.DEFAULT\n",
    "    model = builder_func(weights=weights, box_score_thresh=0.9)\n",
    "    model.eval()\n",
    "    # model.backbone = torch.jit.script(model.backbone) \n",
    "    return model, weights\n",
    "\n",
    "def detection2D(files, output_dir='output', batch_size=5, ):\n",
    "    logger = logging.getLogger(\"detection2D\")\n",
    "    if not os.path.isdir(output_dir): \n",
    "        logger.warning(f\"Output directory '{output_dir}' was not found. Creating directory '{output_dir}'\")\n",
    "        os.mkdir(output_dir)\n",
    "    fnames = [os.path.basename(f) for f in files]\n",
    "    save_files = [os.path.join(output_dir, p) for p in fnames]\n",
    "    imgs = [(decode_image(f), sf) for f, sf in zip(files, save_files)]\n",
    "    batches = batched(imgs, batch_size) \n",
    "    batches = (tuple(zip(*b)) for b in batches)\n",
    "    \n",
    "    logger.info(\"Loading model\")\n",
    "    model, weights = get_model_and_weights(fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights)\n",
    "    preprocess = weights.transforms()\n",
    "    def infer(images):\n",
    "        processed_images = [preprocess(img) for img in images] \n",
    "        return model(processed_images) \n",
    "\n",
    "    logger.info(\"Starting detection\")\n",
    "    for batch in tqdm(batches, total=math.ceil(len(imgs)/batch_size)):\n",
    "        images, save_names = batch\n",
    "        predictions = infer(images) \n",
    "        \n",
    "        # detect_annotate_save(images, save_names, preprocess, model, weights.meta[\"categories\"]) \n",
    "        annotated_images = annotate_batch(images, predictions, weights.meta['categories']) \n",
    "        [img.save(sn) for img, sn in zip(annotated_images, save_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eed22e-03d5-4815-b587-eaa6a852f4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = [os.path.join(\"pics\", f) for f in os.listdir(\"pics\")] \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "detection2D(files[:5],output_dir=\"output\",batch_size=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce74b7-e9de-47a4-af09-d96f64ad481b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Export to PyTorch to ONNX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab4fb1-b088-427a-bf0e-a3f0dbd1516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import onnxscript\n",
    "print(onnxscript.__version__)\n",
    "\n",
    "from onnxscript import opset18  # opset 18 is the latest (and only) supported version for now\n",
    "\n",
    "import onnxruntime\n",
    "print(onnxruntime.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ec2d9-a3ed-4b5f-aed2-83d9a05b4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, wieghts = get_model_and_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a94a75-1cc2-4cb9-85a1-2212394fb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 1000,1000)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"fcos.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['boxes', 'scores', 'labels'], \n",
    "    # output_names=['batch_size'], \n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size', 2: 'height', 3: 'width'},  # Make batch, height, and width dynamic\n",
    "        'output': {0: 'batch_size'}  # Example: make output batch dimension dynamic\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07126e2-04c8-40ba-b35d-fa6662885d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"fcos.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc63e9-c129-4aeb-8f52-4e8a1b1c89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np \n",
    "import torch\n",
    "class ONNX_model: \n",
    "    def __init__(self, model_file): \n",
    "        self.session = ort.InferenceSession(model_file, provider_options=ort.get_available_providers()) # from https://onnxruntime.ai/docs/api/python/tutorial.html\n",
    "    def infer(self, input_img): \n",
    "        input_img = np.asarray(input_img)\n",
    "        input_img = input_img.reshape(1, *input_img.shape)\n",
    "        input_name = self.session.get_inputs()[0].name\n",
    "        output_names = [out.name for out in self.session.get_outputs()] \n",
    "        outputs = self.session.run(output_names, {input_name: input_img}) \n",
    "        return { \n",
    "            name: torch.from_numpy(op) for name, op in zip(output_names, outputs) \n",
    "        }\n",
    "    def __call__(self, input_batch):\n",
    "        return [self.infer(img) for img in input_batch] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5057e09-ea64-4466-9894-477913f4669c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test on individual image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7240344-7aa7-4dc1-893b-335b5e98d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files = [\"pexels-jeffrey-czum-2346165.jpg\", \"pexels-lam-kiên-15008127.jpg\"]\n",
    "# img_files = files[:5] \n",
    "imgs = [decode_image(img_file) for img_file in img_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283fcc26-ab37-4bf8-91ff-028f0e7b0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, weights = get_model_and_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc6736-b8e2-4d56-ab88-d07d68f858c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [weights.transforms()(img) for img in imgs] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84898887-cd18-4a6b-9fc1-253a31f50b2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### native pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a5b88-30f0-4c89-adc8-92324dee5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, weights = get_model_and_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e044d-7962-4254-8710-946ef3aa811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.rand(1,1,1000,1000))[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34108872-be37-4257-b799-261b29749511",
   "metadata": {},
   "source": [
    "### Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2853e-c56d-4066-997e-08ddace4b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ONNX_model('FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V.onnx')\n",
    "model = ONNX_model('fcos.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c6f81-d579-476a-85f9-f63bb1c8d7ce",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed542d-f4ca-471c-8788-38ab837cbe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8bb6b-8adf-4dd7-9cda-ab7e50d86179",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29e727-2a1b-4caf-b7a6-854d42fa42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_imgs = [annotate(img, prediction, weights.meta['categories']) for img, prediction in zip(imgs, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4744d-c560-4d3b-9106-12a86be32710",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "annotated_imgs[i].resize([i//4 for i in annotated_imgs[i].size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83471cbe-c62b-40fc-b5c1-abb14136cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(FCOS_ResNet50_FPN_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328ec14-9c7b-4925-84a4-f09fe0bf1bc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test on Batch of same size image \n",
    "(Incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166a32a-2567-427c-8532-75f5f79fa5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, wieghts = get_model_and_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ed6c1-40dd-47dd-9e8a-0fd478011e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "dummy_input = torch.randn(batch_size, 3, 1000,1000)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"fcos.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['boxes', 'scores', 'labels'], \n",
    "    # output_names=['batch_size'], \n",
    "    dynamic_axes={\n",
    "        'input': {2: 'height', 3: 'width'},  # Make batch, height, and width dynamic\n",
    "        # 'output': {0: 'batch_size'}  # Example: make output batch dimension dynamic\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa4077-19ba-4cb3-8d52-0fc3c79fa4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"fcos.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eaedae-bea9-4fbf-a837-6455ec96001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np \n",
    "import torch\n",
    "class ONNX_model: \n",
    "    def __init__(self, model_file): \n",
    "        self.session = ort.InferenceSession(model_file, provider_options=ort.get_available_providers()) # from https://onnxruntime.ai/docs/api/python/tutorial.html\n",
    "    def infer(self, input_img): \n",
    "        input_img = np.asarray(input_img)\n",
    "        input_img = input_img.reshape(1, *input_img.shape)\n",
    "        input_name = self.session.get_inputs()[0].name\n",
    "        output_names = [out.name for out in self.session.get_outputs()] \n",
    "        outputs = self.session.run(output_names, {input_name: input_img}) \n",
    "        return { \n",
    "            name: torch.from_numpy(op) for name, op in zip(output_names, outputs) \n",
    "        }\n",
    "    def infer_batch(self, input_img): \n",
    "        input_img = np.asarray(input_img)\n",
    "        input_name = self.session.get_inputs()[0].name\n",
    "        output_names = [out.name for out in self.session.get_outputs()] \n",
    "        outputs = self.session.run(output_names, {input_name: input_img}) \n",
    "        return outputs \n",
    "        # return { \n",
    "        #     name: torch.from_numpy(op) for name, op in zip(output_names, outputs) \n",
    "        # }\n",
    "    def __call__(self, input_batch):\n",
    "        return [self.infer(img) for img in input_batch] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583df5e-b009-4da4-b76d-dba315b1741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files = [\"pexels-jeffrey-czum-2346165.jpg\", \"pexels-lam-kiên-15008127.jpg\"]\n",
    "# img_files = files[:5] \n",
    "imgs = [decode_image(img_file) for img_file in img_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3924a8-eefa-42f2-bce1-b74b2a934d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bada0-c7de-4d96-8abd-2fb61fea92ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f4008ff-fac5-4327-8789-d65e612c987e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Combined Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055a80e-101d-4621-a13f-88d3ff7dfd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import logging\n",
    "\n",
    "def get_model_and_weights(builder_func=fcos_resnet50_fpn, weight_class=FCOS_ResNet50_FPN_Weights, get_onnx=False): \n",
    "    logger = logging.getLogger(\"model_and_weights\")\n",
    "    weights = weight_class.DEFAULT\n",
    "    if not get_onnx: \n",
    "        model = builder_func(weights=weights, box_score_thresh=0.9)\n",
    "        model.eval()\n",
    "        return model, weights\n",
    "    else: \n",
    "        model_file = str(weights.DEFAULT)+'.onnx' \n",
    "        if os.path.exists(model_file): \n",
    "            model = ONNX_model(model_file) \n",
    "            return model, weights \n",
    "        else: \n",
    "            logger.info(f\"{model_file} not found, exporting pytorch model to {model_file}\") \n",
    "            model, _ = get_model_and_weights(builder_func, weight_class, get_onnx=False) \n",
    "            dummy_input = torch.randn(1, 3, 1000,1000)\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                dummy_input,\n",
    "                model_file,\n",
    "                export_params=True,\n",
    "                opset_version=11,\n",
    "                do_constant_folding=True,\n",
    "                input_names=['input'],\n",
    "                output_names=list(model(dummy_input)[0].keys()) ,\n",
    "                dynamic_axes={\n",
    "                    'input': {0: 'batch_size', 2: 'height', 3: 'width'},  # Make batch, height, and width dynamic\n",
    "                    'output': {0: 'batch_size'}  # Example: make output batch dimension dynamic\n",
    "                }\n",
    "            )\n",
    "            return get_model_and_weights(builder_func, weight_class, get_onnx=True) \n",
    "            \n",
    "            \n",
    "\n",
    "# def detection2D(files, output_dir='output', batch_size=5, use_onnx=False):\n",
    "#     logger = logging.getLogger(\"detection2D\")\n",
    "#     if not os.path.isdir(output_dir): \n",
    "#         logger.warning(f\"Output directory '{output_dir}' was not found. Creating directory '{output_dir}'\")\n",
    "#         os.mkdir(output_dir)\n",
    "#     fnames = [os.path.basename(f) for f in files]\n",
    "#     save_files = [os.path.join(output_dir, p) for p in fnames]\n",
    "#     imgs = [(decode_image(f), sf) for f, sf in zip(files, save_files)]\n",
    "#     batches = batched(imgs, batch_size) \n",
    "#     batches = (tuple(zip(*b)) for b in batches)\n",
    "    \n",
    "#     logger.info(\"Loading model\")\n",
    "#     model, weights = get_model_and_weights(fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, use_onnx)\n",
    "#     preprocess = weights.transforms()\n",
    "\n",
    "    \n",
    "    \n",
    "#     logger.info(\"Starting detection\")\n",
    "#     for batch in tqdm(batches, total=math.ceil(len(imgs)/batch_size)):\n",
    "#         images, save_names = batch\n",
    "#         detect_annotate_save(images, save_names, preprocess, model, weights.meta[\"categories\"])\n",
    "\n",
    "\n",
    "def detection2D(files, output_dir='output', batch_size=5, use_onnx=False):\n",
    "    logger = logging.getLogger(\"detection2D\")\n",
    "    if not os.path.isdir(output_dir): \n",
    "        logger.warning(f\"Output directory '{output_dir}' was not found. Creating directory '{output_dir}'\")\n",
    "        os.mkdir(output_dir)\n",
    "    fnames = [os.path.basename(f) for f in files]\n",
    "    save_files = [os.path.join(output_dir, p) for p in fnames]\n",
    "    imgs = [(decode_image(f), sf) for f, sf in zip(files, save_files)]\n",
    "    batches = batched(imgs, batch_size) \n",
    "    batches = (tuple(zip(*b)) for b in batches)\n",
    "    \n",
    "    logger.info(\"Loading model\")\n",
    "    model, weights = get_model_and_weights(fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights, use_onnx)\n",
    "    preprocess = weights.transforms()\n",
    "    def infer(images):\n",
    "        processed_images = [preprocess(img) for img in images] \n",
    "        return model(processed_images) \n",
    "\n",
    "    logger.info(\"Starting detection\")\n",
    "    for batch in tqdm(batches, total=math.ceil(len(imgs)/batch_size)):\n",
    "        images, save_names = batch\n",
    "        predictions = infer(images) \n",
    "        \n",
    "        # detect_annotate_save(images, save_names, preprocess, model, weights.meta[\"categories\"]) \n",
    "        annotated_images = annotate_batch(images, predictions, weights.meta['categories']) \n",
    "        [img.save(sn) for img, sn in zip(annotated_images, save_names)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d507b93-ae1c-485a-b716-76d94f51c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(\"pics\", f) for f in os.listdir(\"pics\")] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff95bc9-2ea2-4f02-bf8e-58ebd58c07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "detection2D(files,output_dir=\"output_onnx\",batch_size=2, use_onnx=True) \n",
    "# detection2D(files,output_dir=\"output_torch\",batch_size=2, use_onnx=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fe360-10d2-4869-9175-9abcdf44ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "detection2D(files,output_dir=\"output_torch\",batch_size=2, use_onnx=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5917214-7d3e-480c-85ec-869a02a40504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54347a17-c474-4e73-9e5b-584a6a9d246c",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8496d-276f-4914-923b-89560f39bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from torchvision.io.image import decode_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection import fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "def batched(array, size):\n",
    "    array_iter = iter(array) \n",
    "    while True: \n",
    "        b = []\n",
    "        try: [b.append(next(array_iter)) for _ in range(size)]\n",
    "        except: StopIteration\n",
    "        if b: yield b \n",
    "        else: break\n",
    "\n",
    "def annotate(img, prediction, categories):\n",
    "    labels = [categories[i] for i in prediction[\"labels\"]]\n",
    "    boxes = prediction['boxes']\n",
    "    box = draw_bounding_boxes(\n",
    "        image=img, \n",
    "        boxes=boxes, \n",
    "        labels=labels, \n",
    "        colors=\"red\", \n",
    "        width=4, \n",
    "        font=\"Helvetica.ttf\" if os.path.exists(\"Helvetica.ttf\") else None, \n",
    "        font_size=30\n",
    "    )\n",
    "    im = to_pil_image(box.detach())\n",
    "    return im\n",
    "\n",
    "def annotate_batch(images, predictions, categories, thresh=0.5): \n",
    "    annotated_images = [] \n",
    "    for image, prediction in zip(images, predictions): \n",
    "        prediction = {\n",
    "            k: prediction[k][prediction['scores']>thresh] for k in prediction\n",
    "        }\n",
    "        annotated_images.append(annotate(image, prediction, categories)) \n",
    "    return annotated_images\n",
    "\n",
    "def filter_prediction(prediction, thresh=0.5): \n",
    "    if not isinstance(prediction, dict):\n",
    "        return [filter_predictions(pred, thresh) for pred in predictions]\n",
    "    return {\n",
    "        k: prediction[k][prediction['scores']>thresh] for k in prediction\n",
    "    }\n",
    "\n",
    "\n",
    "class ONNX_model: \n",
    "    def __init__(self, model_file): \n",
    "        self.session = ort.InferenceSession(model_file, provider_options=ort.get_available_providers()) # from https://onnxruntime.ai/docs/api/python/tutorial.html\n",
    "    def infer(self, input_img): \n",
    "        input_img = np.asarray(input_img)\n",
    "        input_img = input_img.reshape(1, *input_img.shape)\n",
    "        input_name = self.session.get_inputs()[0].name\n",
    "        output_names = [out.name for out in self.session.get_outputs()] \n",
    "        outputs = self.session.run(output_names, {input_name: input_img}) \n",
    "        return { \n",
    "            name: torch.from_numpy(op) for name, op in zip(output_names, outputs) \n",
    "        }\n",
    "    def __call__(self, input_batch):\n",
    "        return [self.infer(img) for img in input_batch] \n",
    "\n",
    "\n",
    "def get_model_and_weights(builder_func=fcos_resnet50_fpn, weight_class=FCOS_ResNet50_FPN_Weights, get_onnx=False): \n",
    "    logger = logging.getLogger(\"model_and_weights\")\n",
    "    weights = weight_class.DEFAULT\n",
    "    if not get_onnx: \n",
    "        model = builder_func(weights=weights, box_score_thresh=0.9)\n",
    "        model.eval()\n",
    "        return model, weights\n",
    "    else: \n",
    "        model_file = str(weights.DEFAULT)+'.onnx' \n",
    "        if os.path.exists(model_file): \n",
    "            model = ONNX_model(model_file) \n",
    "            return model, weights \n",
    "        else: \n",
    "            logger.info(f\"{model_file} not found, exporting pytorch model to {model_file}\") \n",
    "            model, _ = get_model_and_weights(builder_func, weight_class, get_onnx=False) \n",
    "            dummy_input = torch.randn(1, 3, 1000,1000)\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                dummy_input,\n",
    "                model_file,\n",
    "                export_params=True,\n",
    "                opset_version=11,\n",
    "                do_constant_folding=True,\n",
    "                input_names=['input'],\n",
    "                output_names=list(model(dummy_input)[0].keys()) ,\n",
    "                dynamic_axes={\n",
    "                    'input': {0: 'batch_size', 2: 'height', 3: 'width'},  # Make batch, height, and width dynamic\n",
    "                    'output': {0: 'batch_size'}  # Example: make output batch dimension dynamic\n",
    "                }\n",
    "            )\n",
    "            return get_model_and_weights(builder_func, weight_class, get_onnx=True) \n",
    "            \n",
    "\n",
    "def detection2D(files, output_dir='output', batch_size=5, builder_and_weights=(fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights), use_onnx=False):\n",
    "    logger = logging.getLogger(\"detection2D\")\n",
    "    if not os.path.isdir(output_dir): \n",
    "        logger.warning(f\"Output directory '{output_dir}' was not found. Creating directory '{output_dir}'\")\n",
    "        os.mkdir(output_dir)\n",
    "    fnames = [os.path.basename(f) for f in files]\n",
    "    save_files = [os.path.join(output_dir, p) for p in fnames]\n",
    "    imgs = [(decode_image(f), sf) for f, sf in zip(files, save_files)]\n",
    "    batches = batched(imgs, batch_size) \n",
    "    batches = (tuple(zip(*b)) for b in batches)\n",
    "    \n",
    "    logger.info(\"Loading model\")\n",
    "    model, weights = get_model_and_weights(*builder_and_weights, use_onnx)\n",
    "    preprocess = weights.transforms()\n",
    "    def infer(images):\n",
    "        processed_images = [preprocess(img) for img in images] \n",
    "        return model(processed_images) \n",
    "\n",
    "    logger.info(\"Starting detection\")\n",
    "    for batch in tqdm(batches, total=math.ceil(len(imgs)/batch_size)):\n",
    "        images, save_names = batch\n",
    "        predictions = infer(images) \n",
    "        \n",
    "        # detect_annotate_save(images, save_names, preprocess, model, weights.meta[\"categories\"]) \n",
    "        annotated_images = annotate_batch(images, predictions, weights.meta['categories']) \n",
    "        [img.save(sn) for img, sn in zip(annotated_images, save_names)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b04981-1837-4f97-bf72-29c99ce4f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(\"pics\", f) for f in os.listdir(\"pics\")] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940d2bd-bc25-47dd-b121-71a89ba6741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "detection2D(files,output_dir=\"output_onnx\",batch_size=2, use_onnx=True) \n",
    "# detection2D(files,output_dir=\"output_torch\",batch_size=2, use_onnx=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ba77b-f0bc-4f41-a2bc-13cfe2ba114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "detection2D(files[:5],output_dir=\"output_onnx_fcos_test\",batch_size=2, builder_and_weights=(fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights), use_onnx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f3f09-ff12-44f1-a5cd-6d51d616250b",
   "metadata": {},
   "source": [
    "# Video Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9516f2-3c60-40c2-bf1a-61b3f1a43073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "def get_infer_method(model, preproc):\n",
    "    def infer(imgs): \n",
    "        proced = [preproc(img) for img in imgs] \n",
    "        return model(proced) \n",
    "    return infer\n",
    "def cv2torch(frame): \n",
    "    return torch.from_numpy(\n",
    "        np.transpose(\n",
    "            cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), \n",
    "            (2,0,1) \n",
    "        )\n",
    "    )\n",
    "def pil2cv2(img): \n",
    "    return cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR)\n",
    "def frame_generator(cap): \n",
    "    ret, frame = cap.read()\n",
    "    while ret: \n",
    "        yield frame\n",
    "        ret, frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82206cf4-7ee1-4f69-8b94-3131bd6528df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_detection2d(video_path, output_path, batch_size=2):\n",
    "    logger = logging.getLogger(\"video_detection2d\")\n",
    "    output_dir = os.path.dirname(output_path) \n",
    "    if not os.path.isdir(output_dir): \n",
    "        logger.info(f\"output dir {output_dir} not found. Creating output dir {output_dir}\") \n",
    "        os.mkdir(output_dir) \n",
    "        \n",
    "    cap = cv2.VideoCapture(video_path) \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT) \n",
    "    # print(cap.get(cv2.CAP_PROP_FRAME_WIDTH),cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "    model, weights = get_model_and_weights(fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights, get_onnx=True) \n",
    "    infer = get_infer_method(model, weights.transforms())\n",
    "    \n",
    "    batched_frames = batched(frame_generator(cap), batch_size)\n",
    "    for i, batch in enumerate(tqdm(batched_frames, total=math.ceil(frame_count/batch_size))): \n",
    "        # batch = next(iter(batched_frames))\n",
    "        if i%2==0: continue\n",
    "        try: \n",
    "            imgs = [cv2torch(img) for img in batch] \n",
    "            preds = infer(imgs) \n",
    "            annotated_batch = annotate_batch(imgs, preds, weights.meta['categories'])\n",
    "            out_frames = [pil2cv2(img) for img in annotated_batch]  \n",
    "            for frame in out_frames: out.write(frame) \n",
    "        except KeyboardInterrupt: \n",
    "            break\n",
    "        \n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f18800-c00f-4dcf-aa32-c062343cf62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_detection2d(\"vids/2048206-hd_1920_1080_30fps.mp4\", \"vids/out.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
